---
title: Moore Money, Moore Problems
slug: moore-money-moore-problems
date_published: 2022-10-08T05:33:47.000Z
date_updated: 2022-10-08T05:37:23.000Z
tags: Blog
---

> "Why cannot we write the entire 24 volumes of the Encyclopedia Brittanica on the head of a pin?"–[There's Plenty of Room at the Bottom](https://web.pa.msu.edu/people/yang/RFeynman_plentySpace.pdf)

In 1959, the physicist Richard Feynman gave a talk explaining the future of miniaturization. In that lecture, he mentions how to put an entire Encyclopedia on the head of a pin. 6 years later, Gordon Moore (co-founder of Intel) wrote an essay predicting the miniaturization of electronics. It was this paragraph that became known as *Moore's Law*:

> "The complexity for minimum component costs has increased at a rate of roughly a factor of two per year. Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years."–[Cramming more components onto integrated circuits](https://newsroom.intel.com/wp-content/uploads/sites/11/2018/05/moores-law-electronics.pdf)

Moore effectively forecasted the steady but rapid rate of computing capability doubling every two years, while also reducing in cost. Not exactly, but here's the scientific theory:

> The number of transistors on a microchip will double every two years.

Moore's Law continued not just for 10, but for 50 years. We're now reaching its threshold. Transistors today have shrunk to less than 100 atoms and billions are on each chip. Billions of microchips exist. By order of magnitude, the number of transistors in existence could equal the amount of sand on Earth.

As the chairman of Intel, Moore gave another talk in 1976 on the design and product strategy for the company:

> "We out a product of given complexity into production; we work on refining the process, eliminating the defects. We gradually move the yield to higher and higher levels. Then we design a still more complex product utilizing all of the improvements, and put that into production. The complexity of our product grows exponentially with time."–Gordon Moore

Of course, microchips alone did not bring a computer revolution. For that, there needed to be new computer designs:

The ENIAC (Electoronic Numerical Integrator and Computer) is considered the first digital computer. It was programmable, electronic, and general purpose. The computer was built by the University of Pennsylvania in 1945, weighing 30 tons, and the size of a small house:
![](__GHOST_URL__/content/images/2022/10/image-5.png)
The first Apple computer, just 30 years later:
![](__GHOST_URL__/content/images/2022/10/image-6.png)
Not sure what the point of my writing this is. I wanted to share my research for an essay I'm working on. Steve Jobs sums it up well:

> "When you first start off trying to solve a problem, the first solutions you come up with are quite complex, and most people stop there. But if you keep going, and live with the problem and peel more layers of the onion off, you can oftentimes arrive at some very elegant and simple solutions. Most people just don't put in the time or energy to get there. "

stay tuned,
rushil.

---

> Consider joining the list of clever people that I share my research with as a data engineer, investor, and optimist :)
